import numpy as np

from keras import backend as K
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Flatten
from keras.layers import Dense
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing import image
# from sklearn.externals import joblib
from keras.models import load_model

TRAIN_DIR = 'chest_xray/train'
VAL_DIR = 'chest_xray/val'
TEST_DIR = 'chest_xray/test'

classifier = Sequential()

# Step 1 and 2 - Convolution and Pooling
classifier.add(Conv2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))
classifier.add(MaxPooling2D(pool_size=(2, 2)))

# Adding a second convolutional layer
classifier.add(Conv2D(32, (3, 3), activation='relu'))
classifier.add(MaxPooling2D(pool_size=(2, 2)))

# Step 3 - Flattening
classifier.add(Flatten())

# Step 4 - Full connection
classifier.add(Dense(units=128, activation='relu'))
classifier.add(Dense(units=1, activation='sigmoid'))

print(classifier.summary())


# Since the classes are not balanced, we cannot use accuracy as a  metric to analyze the model performance
# Metrics F1, precision, and recall have been removed from Keras.
#  So we will use a custom metric function:


def F1(y_true, y_pred):
    def precision(y_true, y_pred):
        """ Batch-wise average precision calculation

        Calculated as tp / (tp + fp), i.e. how many selected items are relevant
        Added epsilon to avoid the Division by 0 exception
        """
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision

    def recall(y_true, y_pred):
        """ Batch-wise average recall calculation

        Computes the Recall metric, or Sensitivity or True Positive Rate
        Calculates as tp / (tp + fn) i.e. how many relevant items are selected

        """
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

    precision = precision(y_true, y_pred)
    recall = recall(y_true, y_pred)
    return 2 * (precision * recall) / (precision + recall + K.epsilon())


# Compiling the CNN
classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=[F1])

# Part 2 - Fitting the CNN to the images

batch_size = 32
train_datagen = ImageDataGenerator(rescale=1. / 255,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1. / 255)

training_set = train_datagen.flow_from_directory(TRAIN_DIR,
                                                 target_size=(64, 64),
                                                 batch_size=batch_size,
                                                 class_mode='binary')

valid_set = test_datagen.flow_from_directory(VAL_DIR,
                                             target_size=(64, 64),
                                             batch_size=batch_size,
                                             class_mode='binary')

#  The instance of ImageDataGenerator().flow_from_directory(...) has an attribute 'filenames'
# which is a list of all the files in the order the generator yields them
n_training_files = len(training_set.filenames)
n_valid_files = len(valid_set.filenames)

# steps_per_epoch parameter: the number of batches of samples it will take to complete one full epoch
# should be equivalent to the total number of samples divided by the batch size.

classifier.fit_generator(training_set,
                         steps_per_epoch=n_training_files / batch_size,
                         epochs=25,
                         validation_data=valid_set,
                         validation_steps=n_valid_files / batch_size)

test_set = test_datagen.flow_from_directory(TEST_DIR,
                                            target_size=(64, 64),
                                            batch_size=batch_size,
                                            class_mode='binary')

test_accuracy = classifier.evaluate_generator(test_set, steps=624)
print('The testing accuracy is :', test_accuracy[1] * 100, '%')

# Part 3 - Making new predictions


test_file_path_1 = 'chest_xray/ill_test.jpeg'
test_file_path_2 = 'chest_xray/norm_test.jpeg'
test_image = image.load_img(test_file_path_2, target_size=(64, 64))
test_image = image.img_to_array(test_image)
test_image = np.expand_dims(test_image, axis=0)
result = classifier.predict(test_image)

label_index_map = (training_set.class_indices)
index_label_map = dict([(value, key) for key, value in label_index_map.items()])

prediction = index_label_map[result[0][0]]
print(prediction)

# Part 4. - Saving model

# joblib_file = "joblib_cnn_model.pkl"
# joblib.dump(classifier, joblib_file)

# cnn_model = joblib.load("joblib_cnn_model.pkl", custom_objects={'metrics': [F1]})
model_path = "cnn_model.h5"
classifier.save(model_path)

# Part 5 - Checking saved model

cnn_model = load_model(model_path, custom_objects={'F1': F1})

test_file_path_1 = 'chest_xray/ill_test.jpeg'
test_file_path_2 = 'chest_xray/norm_test.jpeg'
test_image = image.load_img(test_file_path_1, target_size=(64, 64))
test_image = image.img_to_array(test_image)
test_image = np.expand_dims(test_image, axis=0)
result = cnn_model.predict(test_image)

label_index_map = (training_set.class_indices)
index_label_map = dict([(value, key) for key, value in label_index_map.items()])

prediction = index_label_map[result[0][0]]
print(prediction)